{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcea6dd8",
   "metadata": {},
   "source": [
    "## Web Scraper with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223a98d",
   "metadata": {},
   "source": [
    "#### Python has a built-in module, named urllib, for working with URLs. Add the following code to a new Python file:\n",
    "#### Python, URL'lerle çalışmak için urllib adlı yerleşik bir modüle sahiptir. Aşağıdaki kodu yeni bir Python dosyasına ekleyin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da01728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, site):\n",
    "        self.site = site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e81158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0da92c9a",
   "metadata": {},
   "source": [
    "#### The __init__ method uses a website to extract as a parameter. Later you will pass “https://news.google.com/” as a parameter. The Scraper class has a method called scrape that you will call whenever you want to retrieve data from the site you passed.\n",
    "\n",
    "#### Add the following code to your scrape method:\n",
    "\n",
    "#### __init__ yöntemi, parametre olarak ayıklamak için bir web sitesi kullanır. Daha sonra parametre olarak “https://news.google.com/” geçeceksiniz. Scraper sınıfı, geçtiğiniz siteden veri almak istediğinizde çağıracağınız scrape adında bir metoda sahiptir.\n",
    "\n",
    "#### Kazıma yönteminize aşağıdaki kodu ekleyin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47771721",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32f32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be4b75ed",
   "metadata": {},
   "source": [
    "#### The urlopen () function sends a request to a website and returns a Response object in which its HTML code is stored, along with additional data. The response of the function. read () returns the HTML of the Response object. All the HTML for the website is in the html variable.\n",
    "\n",
    "#### You are now ready to analyze the HTML. Add a new line of code in the scrape function which creates a BeautifulSoup object, and pass the html variable and the “html.parser” string as a parameter:\n",
    "\n",
    "#### urlopen () işlevi bir web sitesine istek gönderir ve ek verilerle birlikte HTML kodunun depolandığı bir Response nesnesi döndürür. Fonksiyonun yanıtı. read (), Response nesnesinin HTML'sini döndürür. Web sitesi için tüm HTML, html değişkenindedir.\n",
    "\n",
    "#### Artık HTML'yi analiz etmeye hazırsınız. Bir BeautifulSoup nesnesi oluşturan scrape işlevine yeni bir kod satırı ekleyin ve html değişkenini ve “html.parser” dizesini parametre olarak iletin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7f2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f65073",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "301dd08a",
   "metadata": {},
   "source": [
    "#### The BeautifulSoup object does all the hard work and parses the HTML. You can now add code to the scrape function that calls the find_all method on the BeautifulSoup object.\n",
    "\n",
    "#### Pass “a” as the parameter and the method will return all the URLs the website is linked to in the HTML code you downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5899f23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)\n",
    "        for tag in sp.find_all(\"a\"):\n",
    "            url = tag.get(\"href\")\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"articles\" in url:\n",
    "                print(\"\\n\" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520d52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b7b198",
   "metadata": {},
   "source": [
    "#### The find_all method returns an iterable containing the tag objects found. Each time around the for loop, the variable receives the value of a new Tag object. Each Tag object has many different instance variables, but you just want the value of the href instance variable, which contains each URL.\n",
    "\n",
    "#### You can get it by calling the get method and passing “href” as a parameter. Finally, you verify that the URL variable contains data; that it contains the string “articles” (you don’t want to print internal links); and if so, you print it. Here is the full web scraper:\n",
    "\n",
    "#### find_all yöntemi, bulunan etiket nesnelerini içeren bir yineleme döndürür. for döngüsü etrafında her seferinde, değişken yeni bir Tag nesnesinin değerini alır. Her Tag nesnesinin birçok farklı örnek değişkeni vardır, ancak siz yalnızca her URL'yi içeren href örnek değişkeninin değerini istiyorsunuz.\n",
    "\n",
    "#### Get yöntemini çağırıp \"href\" parametresini ileterek alabilirsiniz. Son olarak, URL değişkeninin veri içerdiğini doğrularsınız; \"makaleler\" dizesini içerdiğini (dahili bağlantıları yazdırmak istemezsiniz); ve öyleyse, yazdırırsınız. İşte tam web kazıyıcı:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd018e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "./articles/CBMicGh0dHBzOi8vYWJjbmV3cy5nby5jb20vVVMvY2FsaWZvcm5pYXMtcGFqYXJvLXJpdmVyLWJyZWFjaGVzLW92ZXJuaWdodC1yZXNpZGVudHMtdXJnZWQtZXZhY3VhdGUvc3Rvcnk_aWQ9OTc3ODg2NjLSAXRodHRwczovL2FiY25ld3MuZ28uY29tL2FtcC9VUy9jYWxpZm9ybmlhcy1wYWphcm8tcml2ZXItYnJlYWNoZXMtb3Zlcm5pZ2h0LXJlc2lkZW50cy11cmdlZC1ldmFjdWF0ZS9zdG9yeT9pZD05Nzc4ODY2Mg?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMia2h0dHBzOi8vd3d3LmZveG5ld3MuY29tL3VzL2NhbGlmb3JuaWFzLWxldmVlLWJyZWFjaGVkLWF0bW9zcGhlcmljLXJpdmVyLWZvcmNlZC1ldmFjdWF0aW9ucy13YXNoZWQtb3V0LXJvYWRz0gFvaHR0cHM6Ly93d3cuZm94bmV3cy5jb20vdXMvY2FsaWZvcm5pYXMtbGV2ZWUtYnJlYWNoZWQtYXRtb3NwaGVyaWMtcml2ZXItZm9yY2VkLWV2YWN1YXRpb25zLXdhc2hlZC1vdXQtcm9hZHMuYW1w?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiWGh0dHBzOi8vd3d3LnNmZ2F0ZS5jb20vYmF5YXJlYS9hcnRpY2xlL3BhamFyby1yaXZlci1sZXZlZS1icmVha3Mtc2FudGEtY3J1ei0xNzgzMzc2My5waHDSAQA?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiWmh0dHBzOi8vd3d3LnNhbnRhY3J1enNlbnRpbmVsLmNvbS8yMDIzLzAzLzEwL21vdG9yaXN0LXJlc2N1ZWQtZnJvbS13YXRzb252aWxsZS1mbG9vZHdhdGVyc9IBX2h0dHBzOi8vd3d3LnNhbnRhY3J1enNlbnRpbmVsLmNvbS8yMDIzLzAzLzEwL21vdG9yaXN0LXJlc2N1ZWQtZnJvbS13YXRzb252aWxsZS1mbG9vZHdhdGVycy9hbXAv?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiR2h0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8wMy8xMS91cy90ZXhhcy13b21lbi1taXNzaW5nLW1leGljby9pbmRleC5odG1s0gFLaHR0cHM6Ly9hbXAuY25uLmNvbS9jbm4vMjAyMy8wMy8xMS91cy90ZXhhcy13b21lbi1taXNzaW5nLW1leGljby9pbmRleC5odG1s?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiRWh0dHBzOi8vbmV3cy55YWhvby5jb20vMy13b21lbi1taXNzaW5nLW1leGljby1jcm9zc2luZy0wMDQ3NTI3MDkuaHRtbNIBTWh0dHBzOi8vbmV3cy55YWhvby5jb20vYW1waHRtbC8zLXdvbWVuLW1pc3NpbmctbWV4aWNvLWNyb3NzaW5nLTAwNDc1MjcwOS5odG1s?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMic2h0dHBzOi8vd3d3LmZveG5ld3MuY29tL3dvcmxkL3RocmVlLXdvbWVuLW1pc3NpbmctbWV4aWNvLWFmdGVyLWNyb3NzaW5nLWJvcmRlci1mcm9tLXRleGFzLXNlbGwtY2xvdGhpbmctZmxlYS1tYXJrZXTSAXdodHRwczovL3d3dy5mb3huZXdzLmNvbS93b3JsZC90aHJlZS13b21lbi1taXNzaW5nLW1leGljby1hZnRlci1jcm9zc2luZy1ib3JkZXItZnJvbS10ZXhhcy1zZWxsLWNsb3RoaW5nLWZsZWEtbWFya2V0LmFtcA?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiZGh0dHBzOi8vYWJjbmV3cy5nby5jb20vSW50ZXJuYXRpb25hbC8zLXdvbWVuLW1pc3NpbmctMi13ZWVrcy1hZnRlci10cmF2ZWxpbmctdGV4YXMvc3Rvcnk_aWQ9OTc3OTA3NTjSAWhodHRwczovL2FiY25ld3MuZ28uY29tL2FtcC9JbnRlcm5hdGlvbmFsLzMtd29tZW4tbWlzc2luZy0yLXdlZWtzLWFmdGVyLXRyYXZlbGluZy10ZXhhcy9zdG9yeT9pZD05Nzc5MDc1OA?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiXWh0dHBzOi8vd3d3LmNic25ld3MuY29tL25ld3MvY2FsaWZvcm5pYS13ZWF0aGVyLWF0bW9zcGhlcmljLXJpdmVyLWhlYXZ5LXJhaW4tdGhyZWF0LWZsb29kaW5nL9IBYWh0dHBzOi8vd3d3LmNic25ld3MuY29tL2FtcC9uZXdzL2NhbGlmb3JuaWEtd2VhdGhlci1hdG1vc3BoZXJpYy1yaXZlci1oZWF2eS1yYWluLXRocmVhdC1mbG9vZGluZy8?hl=en-US&gl=US&ceid=US%3Aen\n",
      "\n",
      "./articles/CBMiZ2h0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL3NhdWRpLWlyYW4tZGlwbG9tYXRpYy1yZWxhdGlvbnMtbWlkZGxlLWVhc3QtZDc4ZjNjZWFjMDU2MWY0ZmIyMDMwMWU0ZjMyZWVmODjSAQA?hl=en-US&gl=US&ceid=US%3Aen\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Scraper:\n",
    "    def __init__(self, site):\n",
    "        self.site = site\n",
    "\n",
    "    def scrape(self):\n",
    "        r = urllib.request.urlopen(self.site)\n",
    "        html = r.read()\n",
    "        parser = \"html.parser\"\n",
    "        sp = BeautifulSoup(html,parser)\n",
    "        for tag in sp.find_all(\"a\"):\n",
    "            url = tag.get(\"href\")\n",
    "            if url is None:\n",
    "                continue\n",
    "            if \"articles\" in url:\n",
    "                print(\"\\n\" + url)\n",
    "\n",
    "news = \"https://news.google.com/\"\n",
    "Scraper(news).scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe0e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
